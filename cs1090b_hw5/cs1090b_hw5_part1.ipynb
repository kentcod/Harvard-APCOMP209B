{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "id": "RFyNmZ6Dkdun"
   },
   "source": [
    "# ASSIGNMENT CONFIG\n",
    "files:\n",
    "    - data\n",
    "solutions_pdf: false\n",
    "export_cell: false\n",
    "show_question_points: false\n",
    "\n",
    "generate:\n",
    "    pdf: false\n",
    "    pagebreaks: true\n",
    "    zips: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmO29d4ux0Lq"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> Data Science 2: Advanced Topics in Data Science\n",
    "## Homework 5 Part 1: Fine Tuning Llama3\n",
    "\n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2025**<br/>\n",
    "**Instructors**: Pavlos Protopapas, Natesh Pillai, and Chris Gumb\n",
    "\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "TPTLfywkx0Lw",
    "outputId": "c2ea408b-93c1-428d-ce53-05cf88cad644"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RUN THIS CELL\n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/\"\n",
    "    \"content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HaPrtFjfx0L1",
    "outputId": "32fad8bb-f1f0-4f4a-9ec1-8aa53a6560b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.6.0-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\khcod\\micromamba\\envs\\cs109a\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\khcod\\micromamba\\envs\\cs109a\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\khcod\\micromamba\\envs\\cs109a\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\khcod\\micromamba\\envs\\cs109a\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\khcod\\micromamba\\envs\\cs109a\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\khcod\\micromamba\\envs\\cs109a\\lib\\site-packages (from torch) (73.0.1)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\khcod\\micromamba\\envs\\cs109a\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Downloading torch-2.6.0-cp312-cp312-win_amd64.whl (204.1 MB)\n",
      "   ---------------------------------------- 0.0/204.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 2.9/204.1 MB 14.0 MB/s eta 0:00:15\n",
      "    --------------------------------------- 4.7/204.1 MB 13.6 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 7.3/204.1 MB 11.9 MB/s eta 0:00:17\n",
      "   - -------------------------------------- 9.2/204.1 MB 11.0 MB/s eta 0:00:18\n",
      "   -- ------------------------------------- 10.7/204.1 MB 10.2 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 12.1/204.1 MB 9.7 MB/s eta 0:00:20\n",
      "   -- ------------------------------------- 13.4/204.1 MB 9.1 MB/s eta 0:00:21\n",
      "   -- ------------------------------------- 14.4/204.1 MB 8.6 MB/s eta 0:00:22\n",
      "   --- ------------------------------------ 15.5/204.1 MB 8.3 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 16.5/204.1 MB 8.2 MB/s eta 0:00:23\n",
      "   --- ------------------------------------ 17.0/204.1 MB 7.6 MB/s eta 0:00:25\n",
      "   --- ------------------------------------ 17.8/204.1 MB 7.2 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 18.9/204.1 MB 7.0 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 20.2/204.1 MB 6.9 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 21.5/204.1 MB 6.9 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 22.8/204.1 MB 6.8 MB/s eta 0:00:27\n",
      "   ---- ----------------------------------- 24.1/204.1 MB 6.8 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 25.7/204.1 MB 6.8 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 27.0/204.1 MB 6.8 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 28.8/204.1 MB 6.9 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 30.7/204.1 MB 7.0 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 33.0/204.1 MB 7.2 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 35.1/204.1 MB 7.3 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 36.2/204.1 MB 7.2 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 37.5/204.1 MB 7.2 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 38.3/204.1 MB 7.1 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 39.3/204.1 MB 6.9 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 40.1/204.1 MB 6.9 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 41.2/204.1 MB 6.8 MB/s eta 0:00:24\n",
      "   -------- ------------------------------- 41.9/204.1 MB 6.7 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 43.0/204.1 MB 6.6 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 44.3/204.1 MB 6.6 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 45.4/204.1 MB 6.5 MB/s eta 0:00:25\n",
      "   -------- ------------------------------- 45.9/204.1 MB 6.5 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 46.4/204.1 MB 6.4 MB/s eta 0:00:25\n",
      "   --------- ------------------------------ 46.9/204.1 MB 6.2 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 47.2/204.1 MB 6.1 MB/s eta 0:00:26\n",
      "   --------- ------------------------------ 47.7/204.1 MB 6.0 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 48.5/204.1 MB 5.9 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 49.3/204.1 MB 5.9 MB/s eta 0:00:27\n",
      "   --------- ------------------------------ 50.1/204.1 MB 5.8 MB/s eta 0:00:27\n",
      "   ---------- ----------------------------- 51.1/204.1 MB 5.8 MB/s eta 0:00:27\n",
      "   ---------- ----------------------------- 52.4/204.1 MB 5.8 MB/s eta 0:00:27\n",
      "   ---------- ----------------------------- 54.0/204.1 MB 5.8 MB/s eta 0:00:26\n",
      "   ---------- ----------------------------- 55.8/204.1 MB 5.9 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 57.4/204.1 MB 5.9 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 58.2/204.1 MB 5.9 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 59.0/204.1 MB 5.9 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 59.8/204.1 MB 5.8 MB/s eta 0:00:25\n",
      "   ----------- ---------------------------- 60.8/204.1 MB 5.8 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 62.1/204.1 MB 5.8 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 63.7/204.1 MB 5.8 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 65.5/204.1 MB 5.9 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 67.6/204.1 MB 6.0 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 70.0/204.1 MB 6.1 MB/s eta 0:00:23\n",
      "   ------------- -------------------------- 71.0/204.1 MB 6.1 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 72.1/204.1 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 73.4/204.1 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 74.2/204.1 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 75.2/204.1 MB 6.0 MB/s eta 0:00:22\n",
      "   -------------- ------------------------- 76.3/204.1 MB 6.0 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 78.1/204.1 MB 6.0 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 78.9/204.1 MB 6.0 MB/s eta 0:00:21\n",
      "   --------------- ------------------------ 79.7/204.1 MB 5.9 MB/s eta 0:00:22\n",
      "   --------------- ------------------------ 80.5/204.1 MB 5.9 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 81.8/204.1 MB 5.9 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 83.1/204.1 MB 5.9 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 84.9/204.1 MB 5.9 MB/s eta 0:00:21\n",
      "   ----------------- ---------------------- 87.0/204.1 MB 6.0 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 88.1/204.1 MB 6.0 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 89.1/204.1 MB 6.0 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 91.2/204.1 MB 6.0 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 93.1/204.1 MB 6.1 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 95.2/204.1 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 97.3/204.1 MB 6.2 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 98.8/204.1 MB 6.2 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 99.4/204.1 MB 6.2 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 100.1/204.1 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 100.7/204.1 MB 6.1 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 102.0/204.1 MB 6.1 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 103.3/204.1 MB 6.1 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 104.6/204.1 MB 6.1 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 105.9/204.1 MB 6.1 MB/s eta 0:00:17\n",
      "   --------------------- ------------------ 107.2/204.1 MB 6.1 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 108.8/204.1 MB 6.1 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 110.4/204.1 MB 6.1 MB/s eta 0:00:16\n",
      "   ---------------------- ----------------- 112.5/204.1 MB 6.1 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 114.6/204.1 MB 6.2 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 116.7/204.1 MB 6.2 MB/s eta 0:00:15\n",
      "   ----------------------- ---------------- 119.0/204.1 MB 6.3 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 121.4/204.1 MB 6.3 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 122.4/204.1 MB 6.3 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 124.0/204.1 MB 6.3 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 125.0/204.1 MB 6.3 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 125.8/204.1 MB 6.3 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 126.9/204.1 MB 6.3 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 127.9/204.1 MB 6.3 MB/s eta 0:00:13\n",
      "   ------------------------- -------------- 129.2/204.1 MB 6.3 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 130.0/204.1 MB 6.3 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 131.1/204.1 MB 6.2 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 131.9/204.1 MB 6.2 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 132.6/204.1 MB 6.2 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 133.4/204.1 MB 6.2 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 134.2/204.1 MB 6.2 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 135.3/204.1 MB 6.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 136.1/204.1 MB 6.1 MB/s eta 0:00:12\n",
      "   -------------------------- ------------- 136.8/204.1 MB 6.1 MB/s eta 0:00:12\n",
      "   --------------------------- ------------ 138.1/204.1 MB 6.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 139.7/204.1 MB 6.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 141.0/204.1 MB 6.1 MB/s eta 0:00:11\n",
      "   --------------------------- ------------ 142.6/204.1 MB 6.1 MB/s eta 0:00:11\n",
      "   ---------------------------- ----------- 143.7/204.1 MB 6.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 144.7/204.1 MB 6.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 145.8/204.1 MB 6.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 146.8/204.1 MB 6.1 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 147.3/204.1 MB 6.1 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 148.4/204.1 MB 6.0 MB/s eta 0:00:10\n",
      "   ----------------------------- ---------- 149.7/204.1 MB 6.0 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 151.3/204.1 MB 6.1 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 152.8/204.1 MB 6.1 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 153.1/204.1 MB 6.0 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 153.1/204.1 MB 6.0 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 153.1/204.1 MB 6.0 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 153.4/204.1 MB 5.9 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 153.6/204.1 MB 5.9 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 153.6/204.1 MB 5.9 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 153.9/204.1 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 154.4/204.1 MB 5.8 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 154.7/204.1 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 154.9/204.1 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 156.2/204.1 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------ --------- 158.1/204.1 MB 5.7 MB/s eta 0:00:09\n",
      "   ------------------------------- -------- 158.6/204.1 MB 5.7 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 159.1/204.1 MB 5.7 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 159.9/204.1 MB 5.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 160.4/204.1 MB 5.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 161.5/204.1 MB 5.6 MB/s eta 0:00:08\n",
      "   ------------------------------- -------- 162.5/204.1 MB 5.6 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 163.3/204.1 MB 5.6 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 164.1/204.1 MB 5.6 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 164.9/204.1 MB 5.6 MB/s eta 0:00:08\n",
      "   -------------------------------- ------- 165.9/204.1 MB 5.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 166.7/204.1 MB 5.6 MB/s eta 0:00:07\n",
      "   -------------------------------- ------- 167.5/204.1 MB 5.5 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 168.6/204.1 MB 5.5 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 169.9/204.1 MB 5.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 171.2/204.1 MB 5.4 MB/s eta 0:00:07\n",
      "   --------------------------------- ------ 172.8/204.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 173.5/204.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 174.3/204.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 175.6/204.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 176.4/204.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 177.2/204.1 MB 5.4 MB/s eta 0:00:06\n",
      "   ---------------------------------- ----- 178.0/204.1 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 179.0/204.1 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 180.4/204.1 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 181.7/204.1 MB 5.4 MB/s eta 0:00:05\n",
      "   ----------------------------------- ---- 183.5/204.1 MB 5.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 185.1/204.1 MB 5.4 MB/s eta 0:00:04\n",
      "   ------------------------------------ --- 187.2/204.1 MB 5.4 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 189.3/204.1 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 191.6/204.1 MB 5.5 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 193.5/204.1 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 194.8/204.1 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 196.9/204.1 MB 5.5 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 198.7/204.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  200.8/204.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  202.1/204.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.7/204.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.9/204.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  203.9/204.1 MB 5.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 204.1/204.1 MB 5.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 1.8/6.2 MB 8.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.6/6.2 MB 6.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.7/6.2 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 5.0/6.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.2/6.2 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 536.2/536.2 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, torch\n",
      "Successfully installed mpmath-1.3.0 sympy-1.13.1 torch-2.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m softmax\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"
     ]
    }
   ],
   "source": [
    "%pip install torch\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    "    set_seed\n",
    ")\n",
    "from transformers import LlamaPreTrainedModel, LlamaModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from huggingface_hub import notebook_login, login\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "import gc\n",
    "from collections import namedtuple\n",
    "\n",
    "def setup_seed(seed):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.'''\n",
    "    random.seed(seed)\n",
    "    set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # When running on the CuDNN backend, two further options must be set\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # Set a fixed value for the hash seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "setup_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hr_dgNHgkdup",
    "outputId": "4b33c57d-688f-4f8a-8dd4-f8aaac0d6b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.1.0.post303\n",
      "GPU availablity: \n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Some PyTorch settings\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"GPU availablity: \\n{torch.cuda.is_available()}\\n\")\n",
    "\n",
    "TRAIN_CSV = \"data/chatbot_arena_conversations.csv\"\n",
    "model_path = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "lJG4sWrmkdup"
   },
   "outputs": [],
   "source": [
    "# measure notebook runtime\n",
    "time_start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lW7--GrCkdup"
   },
   "source": [
    "<div style = \"background: lightsalmon; border: thin solid black; border-radius: 2px; padding: 5px\">\n",
    "\n",
    "### Instructions\n",
    "- To submit your notebook, follow the instructions given in on the Canvas assignment page.\n",
    "- Plots should be legible and interpretable *without having to refer to the code that generated them*. They should includelabels for the $x$- and $y$-axes as well as a descriptive title and/or legend when appropriate.\n",
    "- When asked to interpret a visualization, do not simply describe it (e.g., \"the curve has a steep slope up\"), but instead explain what you believe the plot *means*.\n",
    "- Autograding tests are mostly to help you debug. The tests are not exhaustive so simply passing all tests may not be sufficient for full credit.\n",
    "- The use of *extremely* inefficient or error-prone code (e.g., copy-pasting nearly identical commands rather than looping) may result in only partial credit.\n",
    "- We have tried to include all the libraries you may need to do the assignment in the imports cell provided below. Please get course staff approval before importing any additional 3rd party libraries.\n",
    "- Enable scrolling output on cells with very long output.\n",
    "- Feel free to add additional code or markdown cells as needed.\n",
    "- Ensure your code runs top to bottom without error and passes all tests by restarting the kernel and running all cells (note that this can take a few minutes).\n",
    "- **You should do a \"Restart Kernel and Run All Cells\" before submitting to ensure (1) your notebook actually runs and (2) all output is visible**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7AgX8bWx0L-"
   },
   "source": [
    "\n",
    "<a id=\"contents\"></a>\n",
    "\n",
    "## Notebook Contents\n",
    "\n",
    "- [**PART 1: Fine Tuning LLM**](#part1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mzGonykqx0L_"
   },
   "source": [
    "## About this Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L555gy_1x0L_"
   },
   "source": [
    "In this homework, we will explore how to fine-tune an LLM for a classification task. In particular we will be fine tuning a [Llama3.2 3B model](https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct)\n",
    "\n",
    "- In [PART 1](#part1), we will begin by building a classifier for [Chatbot Arena Conversations dataset](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations)\n",
    "\n",
    "\n",
    "**IMPORTANT NOTES:**\n",
    "\n",
    "- LLMs are insanely computationally intensive.\n",
    "- **We highly recommend that you train your model on a system using GPUs. For this, we recommend using the [GPU-enabled Jupyter environment](https://ood.huit.harvard.edu/pun/sys/dashboard/batch_connect/sys/ood-jupyterlab-spack-conda/cs1090b/session_contexts/new) provided to you as part of this course.**\n",
    "- Models that take hours to train on CPUs can be trained in just minutes when using GPUs.\n",
    "- **To avoid getting frustrated by having to re-train your models every time you run your notebook, you should save your trained model weights for later use.** Model history dictionaries can also be saved to disk with `pickle` and checked with an `if not` condition. This is a great way to check if the model weights exist before training, preventing redundant retraining. Please, think of the penguins! 🐧\n",
    "\n",
    "**KERNEL CRASHES:**\n",
    "\n",
    "If your kernel crashes as you attempt to train your model, please check the following items:\n",
    "- Models with too many parameters might not fit in GPU memory. Try reducing the size of your model.\n",
    "- A large `batch_size` will attempt to load too many samples in GPU memory. Avoid using a very large batch size.\n",
    "- Avoid creating multiple copies of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOHqvz5Ux0ME",
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "<a id=\"part1\"></a>\n",
    "    \n",
    "<!-- <div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\"> -->\n",
    "\n",
    "# PART 1: Fine-Tuning LLMs (50 Points)\n",
    "\n",
    "\n",
    "<a id=\"part1intro\"></a>\n",
    "\n",
    "## Overview\n",
    "\n",
    "[Return to contents](#contents)\n",
    "\n",
    "In this question, you will work with the [Chatbot Arena Conversations dataset](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations) to build a preference prediction model. Your task is to predict which responses users will prefer when presented with two competing answers generated by different large language models (LLMs). The dataset contains a curated subset of conversations from the [Chatbot Arena platform](https://lmarena.ai/), where human users evaluate responses from various LLMs in head-to-head comparisons. You will fine tune Llama 3.2 3B instruct model that can analyze the user's prompt and the generated responses to determine which response users are more likely to prefer.\n",
    "\n",
    "Learn more about Chatbot Arena here -> [https://arxiv.org/abs/2403.04132](https://arxiv.org/abs/2403.04132)\n",
    "\n",
    "[Llama 3.2 Models](https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices//)\n",
    "\n",
    "**Note:** The dataset contains conversations that may be unsafe, offensive, or upsetting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSuKRAx6kduq"
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<a id=\"q11\"></a>\n",
    "\n",
    "<b>1.1 Initial Preprocessing</b>\n",
    "<hr>\n",
    "\n",
    "<b>Q1.1.1 - Preprocessing</b>\n",
    "\n",
    "<a id=\"q111\"></a>\n",
    "\n",
    "- We provide the code for this part. We will use a separate script for this part - which will be executed just once.\n",
    "```python\n",
    "%%writefile dataset_format.py\n",
    "## preprocessing code here\n",
    "```\n",
    "- `dataset_fomat.py` script does following - \n",
    "\n",
    "- Loads [Chatbot Arena Conversations dataset](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations) using [HuggingFace Dataset](https://huggingface.co/docs/datasets/v3.4.1/en/package_reference/loading_methods#datasets.load_dataset)\n",
    "- Converts dataset to pandas dataframe\n",
    "- Filters dataset for English language only\n",
    "- Separates conversations into - prompt, response_a (this is response from model_a) and response_b (this is response from model_b). Take a closer look at prompts, response_a and response_b - they are all lists, some lists have multiple prompts - these are called multi turn prompts.\n",
    "- To simplify the problem at hand, we drop all rows where there are no clear winners i.e drop rows with winner=='tie'.  Dataset contains much fewer samples where winner==tie.\n",
    "- Keeps only single turn conversations. Multiturn conversations can be very long and much harder to predict correctly.\n",
    "- Apply `utf-8` encoding/decoding for consistency and to avoid any potential tokenization errors.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "D0tc7drEkduq",
    "outputId": "69655f28-a923-4115-fb6a-3b2ab3c1fe56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dataset_format.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile dataset_format.py\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# pip install datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the chatbot arena conversations dataset from HuggingFace\n",
    "dataset = load_dataset(\"lmsys/chatbot_arena_conversations\", token=True)\n",
    "print(dataset)\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = dataset[\"train\"].to_pandas()\n",
    "df = df[df.language=='English'].reset_index(drop=True)\n",
    "print(df.shape)\n",
    "\n",
    "def separate_conversations(conv):\n",
    "    user_texts      = [x['content'] for x in conv if x['role'] == 'user']\n",
    "    assistant_texts = [x['content'] for x in conv if x['role'] == 'assistant']\n",
    "\n",
    "    return user_texts, json.dumps(assistant_texts)\n",
    "\n",
    "\n",
    "df['prompt_a'], df['response_a'] = zip(*df.conversation_a.progress_apply(separate_conversations))\n",
    "df['prompt_b'], df['response_b'] = zip(*df.conversation_b.progress_apply(separate_conversations))\n",
    "\n",
    "\n",
    "assert (df['prompt_a'] == df['prompt_b']).all() == True\n",
    "\n",
    "df['prompt'] = df['prompt_a'].progress_apply(json.dumps)\n",
    "\n",
    "def one_hot_encode(winner):\n",
    "    return pd.Series([int('model_a'==winner), int('model_b'==winner), int('tie'==winner or 'tie (bothbad)'==winner)])\n",
    "\n",
    "df[['winner_model_a', 'winner_model_b', 'winner_tie']] = df.winner.progress_apply(one_hot_encode)\n",
    "\n",
    "cols = ['question_id', 'model_a', 'model_b', 'prompt', 'response_a',\n",
    "        'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "df = pd.DataFrame(df[cols].copy().values, columns=cols).reset_index(drop=True)\n",
    "\n",
    "## Remove ties\n",
    "df = df[df.winner_tie==0].reset_index(drop=True)\n",
    "df.drop(columns=['winner_tie'], inplace=True)\n",
    "\n",
    "df['id'] = df.index\n",
    "\n",
    "df['prompt_length'] = df['prompt'].apply(lambda x: len(eval(x)))\n",
    "df['response_a_length'] = df['response_a'].apply(lambda x: len(eval(x)))\n",
    "df['response_b_length'] = df['response_b'].apply(lambda x: len(eval(x)))\n",
    "\n",
    "df = df[df['prompt_length'] == 1].reset_index(drop=True)\n",
    "\n",
    "df['prompt'] = df['prompt'].apply(lambda x: eval(x)[0])\n",
    "df['response_a'] = df['response_a'].apply(lambda x: eval(x)[0])\n",
    "df['response_b'] = df['response_b'].apply(lambda x: eval(x)[0])\n",
    "\n",
    "df['prompt'] = df['prompt'].apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "df['response_a'] = df['response_a'].apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "df['response_b'] = df['response_b'].apply(lambda x: x.encode('utf-8', 'ignore').decode('utf-8'))\n",
    "\n",
    "df.drop(columns=['prompt_length', 'response_a_length', 'response_b_length'], inplace=True)\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head(1).T)\n",
    "\n",
    "df.to_csv('data/chatbot_arena_conversations.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ORuOr7_ykdur"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('data/chatbot_arena_conversations.csv'):\n",
    "    !mkdir -p data\n",
    "    !python dataset_format.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Df_a0yDtkdur"
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "    \n",
    "<b>Q1.1.2</b>\n",
    "\n",
    "<a id=\"q112\"></a>\n",
    "**Further preprocessing and loading dataset**\n",
    "- Load the dataset from `data/chatbot_arena_conversations.csv` file in a pandas dataframe.\n",
    "- Create binary label `label` from the winner columns (`winner_model_a` or `winner_model_b`). \n",
    "Note: `winner_model_b==1` => `label=1` => Model b is winner.\n",
    "- Perform preliminary EDA \n",
    "- Display dataset shape, `.head()` and target class ratio.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dX0OlDNcx0MF",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "    \n",
    "<b>Q1.1.3 - Filter the dataset</b>\n",
    "\n",
    "<a id=\"q113\"></a>\n",
    "Instantiate tokenizer using `AutoTokenizer`. Analyze the token lengths of prompts and responses using the tokenizer. Filter the dataset to remove very long sequences that might cause memory issues during training. Specifically, limit prompt tokens to less than 100 and both response types to less than 400 tokens i.e `response_a` should be less than 400 tokens and `response_b` should also be less than 400 tokens. \n",
    "Display number of samples in the dataset with `dataframe.shape`\n",
    "\n",
    "Tokenizer Settings -\n",
    "- Be sure to set `tokenizer.pad_token = tokenizer.eos_token`\n",
    "- Also recommended for good practice  `tokenizer.padding_side = \"right\"` and `tokenizer.add_eos_token = True`\n",
    "\n",
    "*Note:* Reason we are limiting the number of tokens - longer texts will require more GPU memory and computation time. Assuming JupyterHub GPU, we believe total maxlen of 1024 tokens per sample should suffice.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nDataset shape after filtering:\", train.shape)\n",
    "print(\"\\nLabel distribution after filtering:\")\n",
    "print(train.label.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXifCpQLkdur"
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "<b>Q1.1.4</b>\n",
    "\n",
    "\n",
    "<a id=\"q114\"></a>\n",
    "- At this point you should have ~14K samples. We are going to randomly sample (to save on training time) 600 samples of each label type ie. 600 samples with winner='model_a' and 600 samples winner='model_b'. Be sure to set random_state=42.\n",
    "\n",
    "- Split the dataset into train and validation set with 80/20 split, stratify with label and random_state=42.\n",
    "\n",
    "- We are also going to sort (ascending=False) the validation set by token length ie. sum prompt_tokens_len, response_a_tokens_len and response_b_tokens_len and then sort.\n",
    "Why should we sort the validation set ? \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining set shape:\", train_df.shape)\n",
    "print(\"Validation set shape:\", val_df.shape)\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sSHvq9mkdur"
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<b>Q1.2</b>\n",
    "\n",
    "<a id=\"q12\">**Creating a Tokenized Dataset for Model Training** </a>\n",
    "\n",
    "Next, you'll create a tokenized dataset using [HuggingFace Datasets](https://huggingface.co/docs/datasets/index) library. To be specific - [from_pandas()](https://huggingface.co/docs/datasets/v3.4.1/en/package_reference/main_classes#datasets.Dataset.from_pandas).\n",
    "\n",
    "However we cannot train model with raw text, we have to tokenize our text and it must return `input_ids`, `attention_mask` and corresponding `label` for every sample.\n",
    "\n",
    "Here we provide you with skeleton, you should fill this appropriately.\n",
    "\n",
    "```python\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    # 1. Format the input text with prompt and both responses. You have the freedom to prepend text with for eg: \"Prompt:\" and/or \"Response A:\" etc.\n",
    "\n",
    "    # Combine the prompt and responses into a single text string\n",
    "    \n",
    "    # 2. Tokenize the text using the tokenizer\n",
    "    # Make sure to set appropriate parameters for padding, truncation, etc. Set max_length to 1024\n",
    "    \n",
    "    # 3. Extract the input_ids and attention_mask\n",
    "    \n",
    "    # 4. Get the label from the example\n",
    "    \n",
    "    return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "def load_data(df, tokenizer):\n",
    "    # 1. Create a Dataset from the pandas DataFrame\n",
    "    raw_datasets = Dataset.from_pandas(...)\n",
    "    \n",
    "    # 2. Apply the tokenize function to each example in the dataset\n",
    "    # Make sure to:\n",
    "    # - Pass the tokenizer to the function\n",
    "    # - Remove original columns that are no longer needed\n",
    "    # - Handle any other necessary parameters\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize,\n",
    "        ...\n",
    "    )\n",
    "\n",
    "    return tokenized_datasets\n",
    "```\n",
    "\n",
    "Expected output :\n",
    "\n",
    "```python\n",
    "print(tokenized_train)\n",
    "print(tokenized_val)\n",
    "```\n",
    "\n",
    "```python\n",
    "Dataset({\n",
    "    features: ['input_ids', 'attention_mask', 'labels'],\n",
    "    num_rows: 960\n",
    "})\n",
    "Dataset({\n",
    "    features: ['input_ids', 'attention_mask', 'labels'],\n",
    "    num_rows: 240\n",
    "})\n",
    "```\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_train)\n",
    "print(tokenized_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5Ux-4bFkdus"
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<b>Q1.3 Setup LoraConfig and BitsAndBytesConfig </b>\n",
    "\n",
    "Fine-tuning large language models like Llama 3.2 can be computationally expensive and memory-intensive. To address this, we utilize parameter-efficient fine-tuning techniques such as LoRA (Low-Rank Adaptation), which allows us to update only a small subset of the model's parameters, significantly reducing memory usage and training time while maintaining performance. Additionally, using quantization through BitsAndBytesConfig enables us to load and train the model with reduced precision (e.g., 4-bit or 8-bit), further optimizing memory usage and speeding up computations, especially with limited resources (GPUs.) \n",
    "\n",
    "<a id=\"q13\"></a>\n",
    "\n",
    "**[Lora Config](https://huggingface.co/docs/peft/en/package_reference/lora#peft.LoraConfig)**\n",
    "\n",
    "We recommend following options -\n",
    "\n",
    "```python\n",
    "r=16,\n",
    "lora_alpha=32,\n",
    "lora_dropout=0.05,\n",
    "bias='none',\n",
    "inference_mode=False,\n",
    "task_type=TaskType.SEQ_CLS,\n",
    "target_modules=['q_proj', 'k_proj', 'v_proj',]\n",
    "```\n",
    "\n",
    "(a) Ask the following question to [Claude.ai](claude.ai) and [gemini.google.com](https://gemini.google.com/). Note: We are asking two AI models, in case one decides to hallucinate. Also recommended reference documentation - https://huggingface.co/docs/peft/main/en/conceptual_guides/lora\n",
    "\n",
    "Explain the significance of following parameters - `r`, `lora_alpha` and `target_modules`\n",
    "\n",
    "Write the answer in your own words - concisely - 3-5 sentences.\n",
    "\n",
    "\n",
    "(b) **Quantization** - Apply 4-bit quantization using BitsAndBytesConfig. Recommended settings -\n",
    "\n",
    "```python\n",
    "load_in_4bit=True,\n",
    "bnb_4bit_compute_dtype=torch.float16,\n",
    "bnb_4bit_quant_type=\"nf4\",\n",
    "```\n",
    "\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "N6AyhgTwkdus"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p64SZoB_kdus"
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<a id=\"q14\"></a>\n",
    "**1.4 Custom Classification Head for LLaMA**\n",
    "\n",
    "In this question, you’ll implement a custom classification head for the pre-trained LLaMA model. Your task is to complete the implementation of the classifier by subclassing LlamaPreTrainedModel.\n",
    "\n",
    "Your custom model should:\n",
    "- Accept inputs (input_ids, attention_mask, labels, etc) and pass them through the pre-trained LLaMA model [LlamaModel](https://huggingface.co/docs/transformers/v4.49.0/en/model_doc/llama#transformers.LlamaModel).\n",
    "- Extract embeddings from the last token of the sequence (based on attention mask).\n",
    "- Classify embeddings into two labels using a linear layer `nn.Linear`.\n",
    "- Output predictions using the Hugging Face standard SequenceClassifierOutput.\n",
    "\n",
    "- Your class will resemble [LlamaForSequenceClassification](https://github.com/huggingface/transformers/blob/a22a4378d97d06b7a1d9abad6e0086d30fdea199/src/transformers/models/llama/modeling_llama.py#L893)\n",
    "\n",
    "- We provide you with the loss function below.\n",
    "\n",
    "\n",
    "Your implementation should match the following structure:\n",
    "\n",
    "```python\n",
    "\n",
    "class CS1090B_LlamaForClassification(LlamaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        pass\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    )\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=pooled_logits,\n",
    "            hidden_states=outputs.hidden_states if output_hidden_states else None,\n",
    "            attentions=outputs.attentions if output_attentions else None,\n",
    "        )\n",
    "\n",
    "```\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sTLNX01ukdus"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your model\n",
    "if 0:\n",
    "    model = CS1090B_LlamaForClassification.from_pretrained(\n",
    "            model_path,\n",
    "            quantization_config=quantization_config,\n",
    "        )\n",
    "\n",
    "    text = \"Hello CS1090B!!\"\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        max_length=16,\n",
    "        truncation=True\n",
    "    )\n",
    "\n",
    "    outputs = model(\n",
    "            input_ids=inputs[\"input_ids\"].to(device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(device)\n",
    "        )\n",
    "    print(outputs.logits)\n",
    "\n",
    "    del model, outputs, inputs, text\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m_gs4M2Nkdus"
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<a id=\"q15\">**Q1.5 Metric and Training** </a>\n",
    "\n",
    "\n",
    "<b>Q1.5.1 - Implementing Evaluation Metrics</b>\n",
    "\n",
    "<a id=\"q151\"></a>\n",
    "\n",
    "Next, we are going to customize [`compute_metrics()`](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.compute_metrics)` which will be utilized when we train our model.\n",
    "\n",
    "Implement a `compute_metrics` function that takes evaluation predictions and calculates performance metrics. This function should:\n",
    "- Take evaluation predictions as input (containing predictions and labels)\n",
    "- Apply softmax to the raw prediction logits\n",
    "- Calculate accuracy by comparing the predicted labels to the true labels\n",
    "- Calculate log loss (cross-entropy) between the predicted probabilities and true labels\n",
    "- Return a dictionary containing both metrics\n",
    "\n",
    "\n",
    "```python\n",
    "def compute_metrics(eval_pred):\n",
    "    # Your code here\n",
    "\n",
    "    return {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"log_loss\": loss\n",
    "        }\n",
    "```\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "gbSbytr0kdus"
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test your compute_metrics function\n",
    "\n",
    "# Simulate predictions (logits) and true labels\n",
    "num_samples = 5\n",
    "num_classes = 2\n",
    "np.random.seed(42)\n",
    "logits = np.random.randn(num_samples, num_classes)\n",
    "labels = np.random.randint(0, num_classes, size=num_samples)\n",
    "\n",
    "# Create a mock object to simulate eval_pred\n",
    "EvalPred = namedtuple(\"EvalPred\", [\"predictions\", \"label_ids\"])\n",
    "eval_pred = EvalPred(predictions=logits, label_ids=labels)\n",
    "\n",
    "print(compute_metrics(eval_pred))\n",
    "del num_samples, num_classes, logits, labels, eval_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-8QF-Cukdus"
   },
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<b>Q1.5.2 - Model Training </b>\n",
    "\n",
    "<a id=\"q152\"></a>\n",
    "\n",
    "\n",
    "Implement a complete training pipeline for the LLaMA classification model using the HuggingFace Trainer API. Your implementation should:\n",
    "\n",
    " - Load and prepare the tokenized datasets created earlier i.e. call `load_data()`\n",
    " - Initialize the CS1090B_LlamaForClassification model with the appropriate quantization configuration\n",
    " - Prepare the model for k-bit training [Link](https://huggingface.co/docs/peft/v0.15.0/en/package_reference/peft_model#peft.prepare_model_for_kbit_training). Recommended setting `use_gradient_checkpointing=False`\n",
    " - Apply the LoRA adapters using the configuration from Q1.3 [Link](https://huggingface.co/docs/peft/v0.15.0/en/package_reference/peft_model#peft.get_peft_model)\n",
    " - Print number of trainable parameters - `model.print_trainable_parameters()`\n",
    " - Configure training arguments with appropriate hyperparameters (learning rate, batch size, etc.) [Link](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments)\n",
    "\n",
    "```python\n",
    " Recommended parameters -\n",
    "\n",
    " training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=4,\n",
    "        warmup_steps=50,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=10,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        push_to_hub=False,\n",
    "        learning_rate=1e-4,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "\n",
    "```\n",
    "\n",
    " - Create a data collator for padding [Link](https://huggingface.co/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorWithPadding)\n",
    "\n",
    " - Initialize the Trainer with the model, training arguments, tokenizer, datasets, data collator and evaluation metrics.\n",
    " - Execute the training process\n",
    " - Save the trained model\n",
    " - Evaluate the model on the validation set and report the results. Validation accuracy should be above 0.6.\n",
    "\n",
    " Note: Pay careful attention to batch sizes and gradient checkpointing settings, as these can significantly impact training time and memory usage. Approx. expected training time on JuputerHub ~35mins.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" style=\"color: #333; background-color: #e8fffb; border-color: #bcfff2; border-width: 1px; border-radius: 3px; padding: 10px;\">\n",
    "\n",
    "<b>Q1.6 - Predict and Display Confusion Matrix </b>\n",
    "\n",
    "<a id=\"q16\"></a>\n",
    "\n",
    "Run the cell below to display confusion matrix on validation set\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `tokenized_val` is a HuggingFace Dataset object\n",
    "output = trainer.predict(tokenized_val)\n",
    "logits = output.predictions  \n",
    "print(logits.shape)\n",
    "\n",
    "predicted_classes = np.argmax(logits, axis=1)\n",
    "\n",
    "true_labels = output.label_ids\n",
    "\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_classes)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion Matrix on Validation Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_end = time.time()\n",
    "print(f\"It took {(time_end - time_start)/60:.2f} minutes for this notebook to run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_SNthpPkdut"
   },
   "source": [
    "**This concludes Part 1 👏**\n",
    "\n",
    "**Please continue to Parts 2 & 3 inthe second notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "cs109a",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
